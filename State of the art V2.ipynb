{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5bbae4",
   "metadata": {
    "id": "4b5bbae4"
   },
   "outputs": [],
   "source": [
    "##### UNCOMMENT #####\n",
    "\n",
    "# # Requirements\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b30ec4b",
   "metadata": {
    "id": "7b30ec4b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e93baf2",
   "metadata": {
    "id": "7e93baf2"
   },
   "outputs": [],
   "source": [
    "path = '../Semantic_chunk_identification' ##### MODIFY #####\n",
    "os.chdir(path)\n",
    "\n",
    "##### UNCOMMENT #####\n",
    "# from google.colab import drive\n",
    "# drive.mount('drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d8772",
   "metadata": {
    "id": "435d8772"
   },
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e61748",
   "metadata": {
    "id": "21e61748"
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a60bb0f",
   "metadata": {
    "id": "7a60bb0f"
   },
   "outputs": [],
   "source": [
    "DATASET = \"headlines\" # or \"images\" or \"answers-students\"\n",
    "\n",
    "if (DATASET == \"headlines\" or DATASET == \"images\") :\n",
    "    ds1_chunked = f'train_2015_10_22.utf-8/STSint.input.{DATASET}.sent1.chunk.txt'  ##### MODIFY #####\n",
    "    ds2_chunked = f'train_2015_10_22.utf-8/STSint.input.{DATASET}.sent2.chunk.txt'  ##### MODIFY #####\n",
    "    ds1 = f'train_2015_10_22.utf-8/STSint.input.{DATASET}.sent1.txt'  ##### MODIFY #####\n",
    "    ds2 = f'train_2015_10_22.utf-8/STSint.input.{DATASET}.sent2.txt'  ##### MODIFY #####\n",
    "else :\n",
    "    ds1_chunked = f'train_students_answers_2015_10_27.utf-8/STSint.input.{DATASET}.sent1.chunk.txt'  ##### MODIFY #####\n",
    "    ds2_chunked = f'train_students_answers_2015_10_27.utf-8/STSint.input.{DATASET}.sent2.chunk.txt'  ##### MODIFY #####\n",
    "    ds1 = f'train_students_answers_2015_10_27.utf-8/STSint.input.{DATASET}.sent1.txt'  ##### MODIFY #####\n",
    "    ds2 = f'train_students_answers_2015_10_27.utf-8/STSint.input.{DATASET}.sent2.txt'  ##### MODIFY #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e6cc049",
   "metadata": {
    "id": "7e6cc049"
   },
   "outputs": [],
   "source": [
    "# Put the sentences in lists\n",
    "\n",
    "ds1_lines = [line.strip() for line in open(ds1)]\n",
    "ds2_lines = [line.strip() for line in open(ds2)]\n",
    "ds1_lines_chunked = [line.strip() for line in open(ds1_chunked)]\n",
    "ds2_lines_chunked = [line.strip() for line in open(ds2_chunked)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f0bfe7",
   "metadata": {
    "id": "d0f0bfe7",
    "outputId": "a5137df4-21f6-4ee1-f04b-d1d4ae67e518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syria peace plan conditions \" unacceptable , \" opposition says\n",
      "[ Syria peace plan conditions ] [ \" ] [ unacceptable ] [ , ] [ \" ] [ opposition ] [ says ]\n"
     ]
    }
   ],
   "source": [
    "# One example\n",
    "\n",
    "idx = 3\n",
    "print(ds1_lines[idx])\n",
    "print(ds1_lines_chunked[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Q5jVyCzAnZ8",
   "metadata": {
    "id": "6Q5jVyCzAnZ8"
   },
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb73e31",
   "metadata": {
    "id": "afb73e31"
   },
   "outputs": [],
   "source": [
    "train_size = int(3/4*len(ds1_lines))\n",
    "\n",
    "ds1_lines_train = ds1_lines[:train_size]\n",
    "ds1_lines_gold_train = ds1_lines_chunked[:train_size]\n",
    "ds1_lines_test = ds1_lines[train_size:]\n",
    "ds1_lines_gold_test = ds1_lines_chunked[train_size:]\n",
    "\n",
    "ds2_lines_train = ds2_lines[:train_size]\n",
    "ds2_lines_gold_train = ds2_lines_chunked[:train_size]\n",
    "ds2_lines_test = ds2_lines[train_size:]\n",
    "ds2_lines_gold_test = ds2_lines_chunked[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b082f245",
   "metadata": {
    "id": "b082f245",
    "outputId": "a5fd6f53-fb66-4cc5-c43e-63b78632a61b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 567\n",
      "Test size: 189\n"
     ]
    }
   ],
   "source": [
    "print('Train size:', len(ds1_lines_train))\n",
    "print('Test size:', len(ds1_lines_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cc546",
   "metadata": {
    "id": "c83cc546"
   },
   "source": [
    "## Chunking with transformers\n",
    "\n",
    "Nous utilisons un pipeline qui tokenise une phrase en entrée, utilise un modèle BERT sur les tokens, et renvoie le chunking de la phrase d'entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fee7b24",
   "metadata": {
    "id": "7fee7b24",
    "outputId": "d6752ae0-7a34-49bc-fa4d-18be4304c904"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\timot\\documents\\1 - centrale marseille\\0.6 - semestre s9\\0_pstaln\\tp\\venv\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-chunk\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-chunk\")\n",
    "pipe = pipeline('ner', grouped_entities=True, model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8VwQ-nuAIv9",
   "metadata": {
    "id": "b8VwQ-nuAIv9"
   },
   "source": [
    "**Méthode 2 pour obtenir les phrases chunkées:** On récupère directement les phrases chunkées en sortie du pipeline.\n",
    "\n",
    "Cependant, ces dernières sont au format minuscule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbeb8ae5",
   "metadata": {
    "id": "dbeb8ae5"
   },
   "outputs": [],
   "source": [
    "# Cette fonction retourne une phrase chunkée par le modèle mais EN MINUSCULES\n",
    "def compute_chunk_list(pipe, sentence):\n",
    "    chunk_dicts = pipe(sentence)\n",
    "    chunk_list = []\n",
    "    for chunk_dict in chunk_dicts:\n",
    "        entity = chunk_dict['word']\n",
    "        entity_group = chunk_dict['entity_group']\n",
    "        chunk_list.append([entity, entity_group])\n",
    "    return chunk_list\n",
    "\n",
    "def get_gold_chunk_list(sentence): # EN MINUSCULES\n",
    "    return sentence.lower().strip(\"[ \").strip(\" ]\").split(\" ] [ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e29f74",
   "metadata": {
    "id": "f3e29f74",
    "outputId": "1859eb59-1f47-4bc6-8554-b07e71cd917d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity_group': 'NP',\n",
       " 'score': 0.9940616,\n",
       " 'word': 'syria peace plan conditions',\n",
       " 'start': 0,\n",
       " 'end': 27}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dicts = pipe('Syria peace plan conditions \" unacceptable , \" opposition says')\n",
    "chunk_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21BkQr0P_1P5",
   "metadata": {
    "id": "21BkQr0P_1P5"
   },
   "source": [
    "**Méthode 1 pour obtenir les phrases chunkées:** On récupère les indexs de début et de fin de chunck pour récupérer les segments de phrase correspondant dans la phrase originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4663c438",
   "metadata": {
    "id": "4663c438"
   },
   "outputs": [],
   "source": [
    "# Cette fonction retourne une phrase chunkée par le modèle COMME IL FAUT\n",
    "def compute_chunk_list2(pipe, sentence):\n",
    "    chunk_dicts = pipe(sentence)\n",
    "    chunk_list = []\n",
    "    for chunk_dict in chunk_dicts:\n",
    "        entity = sentence[chunk_dict['start']:chunk_dict['end']]\n",
    "        entity_group = chunk_dict['entity_group']\n",
    "        chunk_list.append([entity, entity_group])\n",
    "    return chunk_list\n",
    "\n",
    "def get_gold_chunk_list2(sentence): # SANS MINUSCULES\n",
    "    return sentence.strip(\"[ \").strip(\" ]\").split(\" ] [ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a4bf188",
   "metadata": {
    "id": "8a4bf188"
   },
   "outputs": [],
   "source": [
    "ds1_train_chunks2 = [compute_chunk_list2(pipe, sentence) for sentence in ds1_lines_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968df5ca",
   "metadata": {
    "id": "968df5ca",
    "outputId": "af77a835-d3cc-4c56-bb28-0a8f1ea40c76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Former Nazi death camp guard Demjanjuk', 'NP'],\n",
       " ['dead', 'ADJP'],\n",
       " ['at', 'PP'],\n",
       " ['91', 'NP']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1_train_chunks2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68063b32",
   "metadata": {
    "id": "8a4bf188"
   },
   "outputs": [],
   "source": [
    "ds1_chunks = [compute_chunk_list2(pipe, sentence) for sentence in ds1_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "286777b4",
   "metadata": {
    "id": "8a4bf188"
   },
   "outputs": [],
   "source": [
    "ds2_chunks = [compute_chunk_list2(pipe, sentence) for sentence in ds2_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69437278",
   "metadata": {
    "id": "968df5ca",
    "outputId": "af77a835-d3cc-4c56-bb28-0a8f1ea40c76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Former Nazi death camp guard Demjanjuk', 'NP'],\n",
       " ['dead', 'ADJP'],\n",
       " ['at', 'PP'],\n",
       " ['91', 'NP']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b39a49",
   "metadata": {
    "id": "96b39a49"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "On compare le chunking du modèle avec les gold chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6be52aa2",
   "metadata": {
    "id": "6be52aa2"
   },
   "outputs": [],
   "source": [
    "ds1_gold_chunks = [get_gold_chunk_list2(sentence) for sentence in ds1_lines_chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f29034fa",
   "metadata": {
    "id": "6be52aa2"
   },
   "outputs": [],
   "source": [
    "ds2_gold_chunks = [get_gold_chunk_list2(sentence) for sentence in ds2_chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf46aa3b",
   "metadata": {
    "id": "bf46aa3b",
    "outputId": "9e0ae021-7c19-4611-e573-3ca5cd8c5c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Former Nazi death camp guard Demjanjuk', 'NP'], ['dead', 'ADJP'], ['at', 'PP'], ['91', 'NP']]\n",
      "['Former Nazi death camp guard Demjanjuk', 'dead', 'at 91']\n"
     ]
    }
   ],
   "source": [
    "print(ds1_chunks[0])\n",
    "print(ds1_gold_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6d1e4a9",
   "metadata": {
    "id": "c6d1e4a9"
   },
   "outputs": [],
   "source": [
    "# La fonction de score du chunking cherche si chaque chunk de l'output a un gold chunk correspondant.\n",
    "\n",
    "def chunking_score(output_chunks, gold_chunks):\n",
    "    score = 0\n",
    "    for i in range(len(output_chunks)):\n",
    "        sent_score = 0\n",
    "        for chk in output_chunks[i]:\n",
    "            if chk[0] in gold_chunks[i]:\n",
    "                sent_score += 1\n",
    "        score += sent_score/len(gold_chunks[i])\n",
    "#         if i%10==0:\n",
    "#             print(output_chunks[i])\n",
    "#             print(gold_chunks[i])\n",
    "    return score/len(output_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85d76b3b",
   "metadata": {
    "id": "85d76b3b"
   },
   "outputs": [],
   "source": [
    "score = chunking_score(ds1_chunks, ds1_gold_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f56cc6f",
   "metadata": {
    "id": "3f56cc6f",
    "outputId": "32088383-56a0-4d61-8ff5-394f7959005b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5635342574826708"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d256f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_file = open('chunking_scores.txt', 'a')\n",
    "score_file.write(f\"score on {DATASET}: \" + str(score))\n",
    "score_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebc06b",
   "metadata": {
    "id": "6bebc06b"
   },
   "source": [
    "Writing predicted chunking in a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e917bda3",
   "metadata": {
    "id": "e917bda3",
    "outputId": "278ec5b2-b9e6-4897-a2cf-cd63b9df2532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Former Nazi death camp guard Demjanjuk', 'NP'], ['dead', 'ADJP'], ['at', 'PP'], ['91', 'NP']]\n",
      "[ Former Nazi death camp guard Demjanjuk ] [ dead ] [ at ] [ 91 ]\n",
      "\n",
      "[ Former Nazi death camp guard Demjanjuk ] [ dead ] [ at 91 ]\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "sentence = ds1_train_chunks2[0]\n",
    "print(sentence)\n",
    "\n",
    "string = \"[ \"\n",
    "for i in range(len(sentence)-1):\n",
    "    string = string + sentence[i][0] + \" ] [ \"\n",
    "string = string + sentence[-1][0] + \" ]\\n\"\n",
    "print(string)\n",
    "print(ds1_lines_gold_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "425430a1",
   "metadata": {
    "id": "425430a1"
   },
   "outputs": [],
   "source": [
    "chunking_file1 = open(f\"{DATASET}_sent1_predicted_chunks.txt\",\"w\")\n",
    "chunking_file2 = open(f\"{DATASET}_sent2_predicted_chunks.txt\",\"w\")\n",
    "\n",
    "for sentence1, sentence2 in zip(ds1_chunks, ds2_chunks):\n",
    "    string1 = \"[ \"\n",
    "    string2 = \"[ \"\n",
    "    for i in range(len(sentence1)-1):\n",
    "        string1 = string1 + sentence1[i][0] + \" ] [ \"\n",
    "    for i in range(len(sentence2)-1):\n",
    "        string2 = string2 + sentence2[i][0] + \" ] [ \"\n",
    "    string1 = string1 + sentence1[-1][0] + \" ]\\n\"\n",
    "    chunking_file1.write(string1)\n",
    "    string2 = string2 + sentence2[-1][0] + \" ]\\n\"\n",
    "    chunking_file2.write(string2)\n",
    "    \n",
    "chunking_file1.close()\n",
    "chunking_file2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DCnuyPMRSYEG",
   "metadata": {
    "id": "DCnuyPMRSYEG"
   },
   "source": [
    "# Alignement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "udgIkrpTTdA7",
   "metadata": {
    "id": "udgIkrpTTdA7"
   },
   "source": [
    "### Définition des méthodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "701F4RjgTxIE",
   "metadata": {
    "id": "701F4RjgTxIE"
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def norm(x):\n",
    "    norm=0\n",
    "    for elt in x:\n",
    "        norm+=elt**2\n",
    "    norm=sqrt(norm)\n",
    "    return(norm)\n",
    "\n",
    "def similarity(x,y):\n",
    "    '''\n",
    "    Cosine Similarity\n",
    "    '''\n",
    "    sim=np.dot(x, y)\n",
    "    sim=sim/(norm(x)*norm(y))\n",
    "    return(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X2if93NkTgH5",
   "metadata": {
    "id": "X2if93NkTgH5"
   },
   "source": [
    "Méthode d'alignement : Hungarian Algorithm (a.k.a. the Kuhn-Munkres algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "lFMEkA8qTWGT",
   "metadata": {
    "id": "lFMEkA8qTWGT"
   },
   "outputs": [],
   "source": [
    "def alignment_chunks(sentence_1, sentence_2):\n",
    "    '''\n",
    "    Hungarian Algorithm : aligner chunks pour meilleur score possible de phrase \n",
    "\n",
    "    Idea for improvement : \n",
    "    Normalize chunk similarity by number of tokens in shorter chunk \n",
    "    such that it assigned higher scores to pairs of chunks such as physician and general physician.\n",
    "    '''\n",
    "    cost=[]\n",
    "    if len(sentence_2)<len(sentence_1): ##If more rows than columns, not every row needs to be assigned to a column, and vice versa.\n",
    "        s1, s2 = sentence_2, sentence_1 #Always select shorter sentence as first sentence !\n",
    "        inv=True\n",
    "    else:\n",
    "        s1, s2=sentence_1, sentence_2\n",
    "        inv=False\n",
    "\n",
    "    for i in range (len(s1)):\n",
    "        inter=[]\n",
    "        for j in range(len(s2)):\n",
    "            sim=alignment_mots(s1[i], s2[j])        \n",
    "            inter.append(sim)\n",
    "        cost.append(inter)\n",
    "    cost=np.array(cost)\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(-cost) # - car linear_sum_assignment minimise normalement\n",
    "    \n",
    "    #Liste de score de similarité pour chaque alignement optimal de chunks\n",
    "    sim=cost[row_ind, col_ind]\n",
    "    sim=min_max_scaler(sim)\n",
    "\n",
    "    if inv:\n",
    "        row_ind, col_ind=col_ind, row_ind\n",
    "\n",
    "    list_couples_chunks_et_score=[]\n",
    "    for i in range(len(sim)):\n",
    "        list_couples_chunks_et_score.append((row_ind[i], col_ind[i], sim[i]))\n",
    "\n",
    "    return(list_couples_chunks_et_score)\n",
    "\n",
    "\n",
    "def alignment_mots(chunk_1, chunk_2):\n",
    "    '''\n",
    "    Hungarian Algorithm sur 2 chunks : aligner mots pour meilleur score possible\n",
    "    '''\n",
    "    if len(chunk_2)<len(chunk_1): ##If more rows than columns, not every row needs to be assigned to a column, and vice versa.\n",
    "        c1, c2 = chunk_2, chunk_1 #Always select shorter sentence as first sentence !\n",
    "    else:\n",
    "        c1, c2 = chunk_1, chunk_2\n",
    "    cost=[]\n",
    "    for i in range (len(c1)):\n",
    "        inter=[]\n",
    "        for j in range(len(c2)):\n",
    "            inter.append(similarity(c1[i], c2[j]))\n",
    "        cost.append(inter)\n",
    "    cost=np.array(cost)\n",
    "\n",
    "    # print(cost.shape)\n",
    "    # print(chunk_1, chunk_2)\n",
    "    # print()\n",
    "\n",
    "    #col_ind, #col_ind donne alignement des mots pour obtenir meilleur score de similarité possible\n",
    "    row_ind, col_ind = linear_sum_assignment(-cost) #- car linear_sum_assignment minimise normalement\n",
    "    \n",
    "    #score de similarité entre 2 chunks\n",
    "    sim=cost[row_ind, col_ind].sum()\n",
    "\n",
    "    return(sim)\n",
    " \n",
    "\n",
    "def min_max_scaler(sim):\n",
    "    scaled=[]\n",
    "    a, b = 3, 1 #On remarque que les scores non scalés sont tous entre 1 et 3 #np.max(sim), np.min(sim) \n",
    "    \n",
    "    for elt in sim:\n",
    "        inter = (elt-b)/(a-b)\n",
    "        inter = inter*5 #Pour avoir des scores entre 0 et 5\n",
    "        inter = round(inter) #Pour avoir des scores entiers\n",
    "        scaled.append(inter)\n",
    "    return(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68e242e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae956216",
   "metadata": {
    "id": "jNs2oIEYT__G"
   },
   "outputs": [],
   "source": [
    "def create_list_of_embedded_chunks_exploitable_for_alignement(chunked_sentence):\n",
    "  chunk=''\n",
    "\n",
    "  list_sentence=[]\n",
    "  for i in range(len(chunked_sentence)):\n",
    "    elt = chunked_sentence[i]\n",
    "    if elt!='[' or elt !=']' or elt!=' ':\n",
    "      chunk += elt\n",
    "    if elt==']':\n",
    "      chunk = removeNoise(chunk)\n",
    "      list_sentence.append(chunk)\n",
    "      chunk = ''\n",
    "\n",
    "  list_sentence_embedded=[]\n",
    "  for elt in list_sentence:\n",
    "    inter = elt.split(' ')\n",
    "    inter_copy=inter[:] #inter without '\n",
    "\n",
    "    for elt_bis in inter_copy:\n",
    "      if elt_bis=='':\n",
    "        inter_copy.remove(elt_bis)\n",
    "\n",
    "    inter_embedded=[]\n",
    "    for elt_ter in inter_copy:\n",
    "      elt = sbert_model.encode(elt_ter)\n",
    "#       elt = elt.numpy()\n",
    "      elt = list(elt)\n",
    "      if len(elt)!=0:\n",
    "        inter_embedded.append(elt)\n",
    "\n",
    "    list_sentence_embedded.append(inter_embedded)\n",
    "\n",
    "  return(list_sentence_embedded)\n",
    "\n",
    "def removeNoise(line):\n",
    "    noiseTokens = ['[', ']', ',', '.', ':', ';', '\"', \"'\", \"''\", \"`\", \"-\",]\n",
    "    for token in noiseTokens:\n",
    "      line = line.replace(token, \"\")\n",
    "    return line\n",
    "\n",
    "# def my_cosine_similarity(chunk1, liste_chunck_comparables):\n",
    "#   nombre_chunk_comparables = len(liste_chunck_comparables)\n",
    "#   sentences = [chunk1[0]]\n",
    "#   sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "#   for i in range(nombre_chunk_comparables):\n",
    "#     print(liste_chunck_comparables[i][0])\n",
    "#     sentences.append(liste_chunck_comparables[i][0])\n",
    "#   sentence_embeddings = sbert_model.encode(sentences)\n",
    "#   cos_sim = cosine_similarity(sentence_embeddings)\n",
    "#   return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dd6H3JoolfvL",
   "metadata": {
    "id": "Dd6H3JoolfvL"
   },
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3X7w6tJWnOxZ",
   "metadata": {
    "id": "3X7w6tJWnOxZ"
   },
   "source": [
    "Alignement sur les gold chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5184f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_word_with_indexe(sentence, wa_file):\n",
    "  list_words=[]\n",
    "  a=sentence.split(']')\n",
    "  counter=1\n",
    "  for i in range(len(a)):\n",
    "    b=a[i]\n",
    "    b=b.replace('[','')\n",
    "    indices=[]\n",
    "    for elt in b.split():\n",
    "      wa_file.write(str(counter) + ' ' + elt + ' :\\n')\n",
    "      indices.append(counter)\n",
    "      counter+=1\n",
    "    list_words.append(b.split()+[indices])\n",
    "  return(list_words)\n",
    "\n",
    "def write_wa_file(idx, wa_file, sentence_1, sentence_2):\n",
    "\n",
    "    sentence_a=create_list_of_embedded_chunks_exploitable_for_alignement(sentence_1)\n",
    "    sentence_b=create_list_of_embedded_chunks_exploitable_for_alignement(sentence_2)\n",
    "\n",
    "    str_liste_indices_token_chunk1, str_liste_indices_token_chunk2, scores=[], [], []\n",
    "    for elt in alignment_chunks(sentence_a, sentence_b):\n",
    "      str_liste_indices_token_chunk1.append(elt[0])\n",
    "      str_liste_indices_token_chunk2.append(elt[1])\n",
    "      scores.append(elt[2])\n",
    "\n",
    "    wa_file.write('<sentence id=\"' + str(idx) + '\" status=\"\">\\n')\n",
    "    wa_file.write('// ' + sentence_1 + '\\n')\n",
    "    wa_file.write('// ' + sentence_2 + '\\n')\n",
    "    wa_file.write('<source>\\n')\n",
    "    chunks1=enumerate_word_with_indexe(sentence_1, wa_file)\n",
    "    wa_file.write('</source>\\n')\n",
    "    wa_file.write('<translation>\\n')\n",
    "    chunks2=enumerate_word_with_indexe(sentence_2, wa_file)\n",
    "    wa_file.write('</translation>\\n')\n",
    "    wa_file.write('<alignment>\\n')\n",
    "    for i in range(len(scores)): \n",
    "      a, b=str_liste_indices_token_chunk1[i], str_liste_indices_token_chunk2[i]\n",
    "      wa_file.write(str(chunks1[a][-1]) + ' <==> ' + str(chunks2[b][-1])\n",
    "          + ' // EQUI // ' + str(scores[i]) + ' // ' + str(chunks1[a][:-1]) + ' <==> ' + str(chunks2[b][:-1]) + '\\n')\n",
    "    wa_file.write('</alignment>\\n')\n",
    "    wa_file.write('</sentence>\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c292ae5",
   "metadata": {},
   "source": [
    "Test avant d'enregistrer les alignements dans un fichier .wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "DIaBZr-elhXJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "executionInfo": {
     "elapsed": 549,
     "status": "error",
     "timestamp": 1642014887702,
     "user": {
      "displayName": "William Jonas",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07205973325962472450"
     },
     "user_tz": -60
    },
    "id": "DIaBZr-elhXJ",
    "outputId": "8f1c7ac5-f0a0-4857-ec61-341a47d68514"
   },
   "outputs": [],
   "source": [
    "dataset1_chunked = ds1_lines_chunked\n",
    "dataset2_chunked = ds2_lines_chunked\n",
    "\n",
    "alignements_gold = []\n",
    "for S1, S2 in zip(dataset1_chunked[:10], dataset2_chunked[:10]): \n",
    "\n",
    "  #Preprocess\n",
    "  s1 = create_list_of_embedded_chunks_exploitable_for_alignement(S1)\n",
    "  s2 = create_list_of_embedded_chunks_exploitable_for_alignement(S2)\n",
    "\n",
    "  score_line = alignment_chunks(s1, s2)\n",
    "  alignements_gold.append(score_line)\n",
    "\n",
    "#alignements_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69122410",
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_file = open(f\"{DATASET}_predicted_alignments_on_gold_chunks.wa\",\"w\")\n",
    "\n",
    "idx = 1\n",
    "for S1, S2 in zip(dataset1_chunked, dataset2_chunked): \n",
    "    write_wa_file(idx, wa_file, S1, S2)\n",
    "    idx+=1\n",
    "\n",
    "wa_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YvBf4zCsnSLi",
   "metadata": {
    "id": "YvBf4zCsnSLi"
   },
   "source": [
    "Alignement sur les chunks prédits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_predicted = f'{DATASET}_sent1_predicted_chunks.txt'  ##### MODIFY #####\n",
    "ds2_predicted = f'{DATASET}_sent2_predicted_chunks.txt'  ##### MODIFY #####\n",
    "\n",
    "chunkedLines1 = [line.strip() for line in open(ds1_predicted)]\n",
    "chunkedLines2 = [line.strip() for line in open(ds2_predicted)]\n",
    "\n",
    "wa_file = open(f\"{DATASET}_predicted_alignments_on_predicted_chunks.wa\",\"w\")\n",
    "\n",
    "idx = 1\n",
    "for S1, S2 in zip(chunkedLines1, chunkedLines2): \n",
    "    write_wa_file(idx, wa_file, S1, S2)\n",
    "    idx+=1\n",
    "\n",
    "wa_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e25a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "21e61748",
    "c83cc546",
    "DCnuyPMRSYEG",
    "udgIkrpTTdA7",
    "euTfkt6cayVY"
   ],
   "name": "State of the art.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
