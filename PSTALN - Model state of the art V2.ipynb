{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5bbae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### UNCOMMENT #####\n",
    "\n",
    "# # Requirements\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b30ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e93baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Semantic_chunk_identification' ##### MODIFY #####\n",
    "os.chdir(path)\n",
    "\n",
    "##### UNCOMMENT #####\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d8772",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e61748",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a60bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"headlines\" # or \"images\" or \"answers-students\"\n",
    "\n",
    "if (DATASET == \"headlines\" or DATASET == \"images\") :\n",
    "    ds1_chunked = f'train_2015_10_22.utf-8/STSint.input.{DATASET}.sent1.chunk.txt'  ##### MODIFY #####\n",
    "    ds2_chunked = f'train_2015_10_22.utf-8/STSint.input.{DATASET}.sent2.chunk.txt'  ##### MODIFY #####\n",
    "    ds1 = f'train_2015_10_22.utf-8/STSint.input.{DATASET}.sent1.txt'  ##### MODIFY #####\n",
    "    ds2 = f'train_2015_10_22.utf-8/STSint.input.{DATASET}.sent2.txt'  ##### MODIFY #####\n",
    "else :\n",
    "    ds1_chunked = f'train_students_answers_2015_10_27.utf-8/STSint.input.{DATASET}.sent1.chunk.txt'  ##### MODIFY #####\n",
    "    ds2_chunked = f'train_students_answers_2015_10_27.utf-8/STSint.input.{DATASET}.sent2.chunk.txt'  ##### MODIFY #####\n",
    "    ds1 = f'train_students_answers_2015_10_27.utf-8/STSint.input.{DATASET}.sent1.txt'  ##### MODIFY #####\n",
    "    ds2 = f'train_students_answers_2015_10_27.utf-8/STSint.input.{DATASET}.sent2.txt'  ##### MODIFY #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6cc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the sentences in lists\n",
    "\n",
    "ds1_lines = [line.strip() for line in open(ds1)]\n",
    "ds2_lines = [line.strip() for line in open(ds2)]\n",
    "ds1_lines_chunked = [line.strip() for line in open(ds1_chunked)]\n",
    "ds2_lines_chunked = [line.strip() for line in open(ds2_chunked)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f0bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syria peace plan conditions \" unacceptable , \" opposition says\n",
      "[ Syria peace plan conditions ] [ \" ] [ unacceptable ] [ , ] [ \" ] [ opposition ] [ says ]\n"
     ]
    }
   ],
   "source": [
    "# One example\n",
    "\n",
    "idx = 3\n",
    "print(ds1_lines[idx])\n",
    "print(ds1_lines_chunked[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afb73e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(3/4*len(ds1_lines))\n",
    "\n",
    "ds1_lines_train = ds1_lines[:train_size]\n",
    "ds1_lines_gold_train = ds1_lines_chunked[:train_size]\n",
    "ds1_lines_test = ds1_lines[train_size:]\n",
    "ds1_lines_gold_test = ds1_lines_chunked[train_size:]\n",
    "\n",
    "ds2_lines_train = ds2_lines[:train_size]\n",
    "ds2_lines_gold_train = ds2_lines_chunked[:train_size]\n",
    "ds2_lines_test = ds2_lines[train_size:]\n",
    "ds2_lines_gold_test = ds2_lines_chunked[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b082f245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 567\n",
      "Test size: 189\n"
     ]
    }
   ],
   "source": [
    "print('Train size:', len(ds1_lines_train))\n",
    "print('Test size:', len(ds1_lines_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cc546",
   "metadata": {},
   "source": [
    "## Chunking with transformers\n",
    "\n",
    "Nous utilisons un pipeline qui tokenise une phrase en entrée, utilise un modèle BERT sur les tokens, et renvoie le chunking de la phrase d'entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fee7b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\timot\\documents\\1 - centrale marseille\\0.6 - semestre s9\\0_pstaln\\tp\\venv\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-chunk\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-chunk\")\n",
    "pipe = pipeline('ner', grouped_entities=True, model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbeb8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction retourne une phrase chunkée par le modèle mais EN MINUSCULES\n",
    "def compute_chunk_list(pipe, sentence):\n",
    "    chunk_dicts = pipe(sentence)\n",
    "    chunk_list = []\n",
    "    for chunk_dict in chunk_dicts:\n",
    "        entity = chunk_dict['word']\n",
    "        entity_group = chunk_dict['entity_group']\n",
    "        chunk_list.append([entity, entity_group])\n",
    "    return chunk_list\n",
    "\n",
    "def get_gold_chunk_list(sentence): # EN MINUSCULES\n",
    "    return sentence.lower().strip(\"[ \").strip(\" ]\").split(\" ] [ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3e29f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity_group': 'NP',\n",
       " 'score': 0.9940616,\n",
       " 'word': 'syria peace plan conditions',\n",
       " 'start': 0,\n",
       " 'end': 27}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dicts = pipe('Syria peace plan conditions \" unacceptable , \" opposition says')\n",
    "chunk_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4663c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction retourne une phrase chunkée par le modèle COMME IL FAUT\n",
    "def compute_chunk_list2(pipe, sentence):\n",
    "    chunk_dicts = pipe(sentence)\n",
    "    chunk_list = []\n",
    "    for chunk_dict in chunk_dicts:\n",
    "        entity = sentence[chunk_dict['start']:chunk_dict['end']]\n",
    "        entity_group = chunk_dict['entity_group']\n",
    "        chunk_list.append([entity, entity_group])\n",
    "    return chunk_list\n",
    "\n",
    "def get_gold_chunk_list2(sentence): # SANS MINUSCULES\n",
    "    return sentence.strip(\"[ \").strip(\" ]\").split(\" ] [ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a4bf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_train_chunks2 = [compute_chunk_list2(pipe, sentence) for sentence in ds1_lines_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "968df5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Former Nazi death camp guard Demjanjuk', 'NP'],\n",
       " ['dead', 'ADJP'],\n",
       " ['at', 'PP'],\n",
       " ['91', 'NP']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1_train_chunks2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b39a49",
   "metadata": {},
   "source": [
    "Ici, on compare le chunking du modèle avec les gold chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6be52aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_train_gold_chunks2 = [get_gold_chunk_list2(sentence) for sentence in ds1_lines_gold_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf46aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Former Nazi death camp guard Demjanjuk', 'NP'], ['dead', 'ADJP'], ['at', 'PP'], ['91', 'NP']]\n",
      "['Former Nazi death camp guard Demjanjuk', 'dead', 'at 91']\n"
     ]
    }
   ],
   "source": [
    "print(ds1_train_chunks2[0])\n",
    "print(ds1_train_gold_chunks2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6d1e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction de score du chunking cherche si chaque chunk de l'output a un gold chunk correspondant.\n",
    "\n",
    "def chunking_score(output_chunks, gold_chunks):\n",
    "    score = 0\n",
    "    for i in range(len(output_chunks)):\n",
    "        sent_score = 0\n",
    "        for chk in output_chunks[i]:\n",
    "            if chk[0] in gold_chunks[i]:\n",
    "                sent_score += 1\n",
    "        score += sent_score/len(gold_chunks[i])\n",
    "#         if i%10==0:\n",
    "#             print(output_chunks[i])\n",
    "#             print(gold_chunks[i])\n",
    "    return score/len(output_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85d76b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = chunking_score(ds1_train_chunks2, ds1_train_gold_chunks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f56cc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5704308282350613"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebc06b",
   "metadata": {},
   "source": [
    "## Writing predicted chunking in a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e917bda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Former Nazi death camp guard Demjanjuk', 'NP'], ['dead', 'ADJP'], ['at', 'PP'], ['91', 'NP']]\n",
      "[ Former Nazi death camp guard Demjanjuk ] [ dead ] [ at ] [ 91 ]\n",
      "\n",
      "[ Former Nazi death camp guard Demjanjuk ] [ dead ] [ at 91 ]\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "sentence = ds1_train_chunks2[0]\n",
    "print(sentence)\n",
    "\n",
    "string = \"[ \"\n",
    "for i in range(len(sentence)-1):\n",
    "    string = string + sentence[i][0] + \" ] [ \"\n",
    "string = string + sentence[-1][0] + \" ]\\n\"\n",
    "print(string)\n",
    "print(ds1_lines_gold_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "425430a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_file = open(f\"{DATASET}_predicted_chunks.txt\",\"w\")\n",
    "\n",
    "for sentence in ds1_train_chunks2:\n",
    "    string = \"[ \"\n",
    "    for i in range(len(sentence)-1):\n",
    "        string = string + sentence[i][0] + \" ] [ \"\n",
    "    string = string + sentence[-1][0] + \" ]\\n\"\n",
    "    chunking_file.write(string)\n",
    "    \n",
    "chunking_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ec404",
   "metadata": {},
   "source": [
    "# Aligning the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def min_max_scaler(sim):\n",
    "    scaled=[]\n",
    "    a, b=1, -1 #Cosine similarity entre -1 et 1\n",
    "    for elt in sim:\n",
    "        inter=(elt-b)/(a-b)\n",
    "        inter=inter*5 #Pour avoir des scores entre 0 et 5\n",
    "        inter=round(inter) #Pour avoir des scores entiers\n",
    "        scaled.append(inter)\n",
    "    return(scaled)\n",
    "\n",
    "def alignment_chunks(cost):\n",
    "    \n",
    "    cost_bis=np.array(cost) #si pas sous bon format\n",
    "    inv=False\n",
    "    if cost_bis.shape[0]<cost_bis.shape[1]:\n",
    "        cost_bis=cost_bis.T\n",
    "        inv=True\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(-cost_bis) #- car linear_sum_assignment minimise normalement\n",
    "    sim=cost_bis[row_ind, col_ind] #Liste de score de similarité pour chaque alignement optimal de chunks\n",
    "\n",
    "    if inv:\n",
    "        row_ind, col_ind=col_ind, row_ind\n",
    "\n",
    "    sim=min_max_scaler(sim)\n",
    "\n",
    "    list_couples_chunks_et_score=[]\n",
    "    for i in range(len(sim)):\n",
    "        list_couples_chunks_et_score.append((row_ind[i], col_ind[i], sim[i]))\n",
    "    return(list_couples_chunks_et_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
